% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ezr_h2o_compare_clf.R
\name{ezr.h2o_compare_clf}
\alias{ezr.h2o_compare_clf}
\title{Title H2o Classifier Model Compare}
\usage{
ezr.h2o_compare_clf(vector_of_models, compare_train = FALSE,
  compare_test = TRUE, test_dataset = NULL, compare_xval = FALSE,
  compare_valid = FALSE, return_full_model_params = FALSE)
}
\arguments{
\item{compare_train}{Get the metrics for train?  Defaults to False}

\item{compare_test}{Get the metrics for test?  Defaults to true.  You need to provide a dataset to evaluate against!}

\item{test_dataset}{The dataset that you will predict against.}

\item{compare_xval}{Get the metrics for XVAL?  Defaults to false.}

\item{compare_valid}{Get the metrics for Validation dataset?  Defautls to False}

\item{return_full_model_params}{False by default.  If FALSE returns only the tuning metrics that were used. If TRUE, will return everything.}
}
\value{
Returns a dataframe of the metrics chosen. if 'all' is chosen on the return_type, then a list of dataframes is returned.
}
\description{
This function makes it easier to compare multiple models side by side.
It takes an input vector of model names and returns the performance metrics for validation dataset, training dataset, test dataset, xval dataset as desired.  Additionally, it
}
\examples{

}
